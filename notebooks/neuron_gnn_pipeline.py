# -*- coding: utf-8 -*-
"""Model 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTbrn70EwFF5l2gbbW18OPy1VuMSXucb
"""

import os
import torch

torch_version = torch.__version__
cuda_version = torch.version.cuda
print(f"PyTorch version: {torch_version}")
print(f"CUDA version: {cuda_version}")

os.environ['TORCH'] = torch_version
os.environ['CUDA'] = f"cu{cuda_version.replace('.', '')}"

print(f"TORCH env var: {os.environ['TORCH']}")
print(f"CUDA env var: {os.environ['CUDA']}")

!pip install pyg-lib -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

from google.colab import drive
drive.mount('/content/drive')

"""
Neuronal Classification System using Graph Neural Networks

Multi-task GNN for predicting neuronal properties from Drosophila connectome data.
"""

import os
import json
import re
import pickle
from typing import Tuple, Dict, Any

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.stats import gaussian_kde
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MultiLabelBinarizer
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, f1_score
from torch_geometric.data import Data
from torch_geometric.nn import NNConv
from torch_geometric.loader import NeighborLoader

# Constants
EMBED_DIM_NODE = 8
EMBED_DIM_EDGE = 4
HIDDEN_DIM = 128
DROPOUT = 0.1
BATCH_SIZE = 512
LEARNING_RATE = 1e-3
EPOCHS = 40
NT_THRESHOLD = 0.98
RANDOM_STATE = 1

# Data paths
DATA_DIR = '/content/drive/MyDrive/tfg/data'
OUTPUT_DIR = '/content/drive/MyDrive/tfg'


def setup_environment():
    """Setup PyTorch and CUDA environment."""
    torch_version = torch.__version__
    cuda_version = torch.version.cuda
    print(f"PyTorch {torch_version}, CUDA {cuda_version}")

    os.environ['TORCH'] = torch_version
    os.environ['CUDA'] = f"cu{cuda_version.replace('.', '')}"

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    return device


def load_datasets() -> Dict[str, Dict[str, Any]]:
    """Load required datasets from CSV files."""
    required_files = [
        "classification.csv", "cell_stats.csv", "neuropil_synapse_table.csv",
        "coordinates.csv", "connections.csv", "neurons.csv",
        "connectivity_tags.csv", "consolidated_cell_types.csv"
    ]

    data = {}
    for filename in required_files:
        name = filename[:-4]
        path = os.path.join(DATA_DIR, filename)
        df = pd.read_csv(path)
        data[name] = {"name": name, "filename": filename, "path": path, "df": df}

    print(f"Loaded {len(data)} datasets")
    return data


def parse_coordinates(df_coordinates: pd.DataFrame) -> pd.DataFrame:
    """Parse position strings and calculate peak coordinates using KDE."""
    xyz = (
        df_coordinates["position"]
        .str.replace(r"[\[\]]", "", regex=True)
        .str.split(expand=True)
        .astype(float)
        .rename(columns={0: "x", 1: "y", 2: "z"})
    )
    xyz["root_id"] = df_coordinates["root_id"].values

    peaks = []
    for rid, grp in xyz.groupby("root_id"):
        coords = grp[["x", "y", "z"]].to_numpy()
        n = len(coords)

        if n == 1:
            peak = coords[0]
        elif n < 3:
            peak = np.median(coords, axis=0)
        else:
            try:
                kde = gaussian_kde(coords.T)
                peak = coords[kde(coords.T).argmax()]
            except np.linalg.LinAlgError:
                peak = np.median(coords, axis=0)

        peaks.append({
            "root_id": rid,
            "centroid_x": peak[0],
            "centroid_y": peak[1],
            "centroid_z": peak[2]
        })

    return pd.DataFrame(peaks)


def encode_categorical(df: pd.DataFrame, column: str, prefix: str) -> pd.DataFrame:
    """Create one-hot encoding for categorical column."""
    dummies = pd.get_dummies(df[column], prefix=prefix).astype(int)
    dummies["root_id"] = df["root_id"].values
    return dummies


def encode_multilabel(df: pd.DataFrame, column: str, sep: str = ",") -> Tuple[pd.DataFrame, MultiLabelBinarizer]:
    """Encode multi-label column using MultiLabelBinarizer."""
    tags_list = df[column].fillna("").apply(
        lambda x: [item.strip() for item in x.split(sep) if item.strip()]
    )
    mlb = MultiLabelBinarizer()
    onehot = mlb.fit_transform(tags_list)
    df_encoded = pd.DataFrame(onehot, columns=[f"tag_{label}" for label in mlb.classes_])
    df_encoded["root_id"] = df["root_id"].values
    return df_encoded.astype(int), mlb


def build_node_features(data: Dict[str, Dict]) -> Tuple[pd.DataFrame, MultiLabelBinarizer]:
    """Build node feature matrix from raw datasets."""
    # Start with neurons that meet NT threshold
    nt_avg_cols = ["da_avg", "ser_avg", "gaba_avg", "glut_avg", "ach_avg", "oct_avg"]
    df_neurons = data["neurons"]["df"]
    df_neurons = df_neurons.loc[df_neurons[nt_avg_cols].sum(axis=1) >= NT_THRESHOLD].copy()

    # Parse neuropil regions
    df_neurons[["input_neuropil", "output_neuropil"]] = df_neurons["group"].str.split(".", expand=True)
    df_neurons["output_neuropil"] = df_neurons["output_neuropil"].fillna(df_neurons["input_neuropil"])

    # Filter valid neurons
    df_neurons = df_neurons[
        (df_neurons["nt_type"].notna()) &
        (df_neurons["input_neuropil"].notna()) &
        (~df_neurons["input_neuropil"].isin(["UNASGD", "NO_CONS"])) &
        (df_neurons["output_neuropil"].notna()) &
        (~df_neurons["output_neuropil"].isin(["UNASGD", "NO_CONS"]))
    ].reset_index(drop=True)

    df_nodes = df_neurons[["root_id", "nt_type", "input_neuropil", "output_neuropil"]].copy()

    # Add coordinates
    coordinates = parse_coordinates(data["coordinates"]["df"])
    df_nodes = df_nodes.merge(coordinates, on="root_id", how="inner")

    # Add primary type
    primary_types = data["consolidated_cell_types"]["df"][["root_id", "primary_type"]]
    df_nodes = df_nodes.merge(primary_types, on="root_id", how="inner")

    # Add connectivity tags (multi-label)
    connectivity_tags = data["connectivity_tags"]["df"][["root_id", "connectivity_tag"]]
    tags_encoded, tag_mlb = encode_multilabel(connectivity_tags, "connectivity_tag")
    df_nodes = df_nodes.merge(tags_encoded, on="root_id", how="inner")
    df_nodes = df_nodes.merge(connectivity_tags, on="root_id", how="inner")

    # Add cell stats
    cell_stats = data["cell_stats"]["df"][["root_id", "length_nm", "size_nm", "area_nm"]]
    df_nodes = df_nodes.merge(cell_stats, on="root_id", how="inner")

    # Add classification features
    classification = data["classification"]["df"]
    side_encoded = encode_categorical(classification, "side", "side")
    flow_encoded = encode_categorical(classification, "flow", "flow")
    df_nodes = df_nodes.merge(side_encoded, on="root_id", how="inner")
    df_nodes = df_nodes.merge(flow_encoded, on="root_id", how="inner")
    df_nodes = df_nodes.merge(classification[["root_id", "super_class"]], on="root_id", how="inner")

    # Add synapse counts
    synapse_data = data["neuropil_synapse_table"]["df"]
    rename_dict = {
        'input synapses': 'input_synapses_count',
        'input partners': 'input_partners_count',
        'output synapses': 'output_synapses_count',
        'output partners': 'output_partners_count'
    }
    synapse_data = synapse_data.rename(columns=rename_dict)
    df_nodes = df_nodes.merge(synapse_data[list(rename_dict.values()) + ["root_id"]], on="root_id", how="inner")

    # Add node IDs
    unique_root_ids = df_nodes["root_id"].tolist()
    id2idx = {rid: i for i, rid in enumerate(unique_root_ids)}
    df_nodes["nid"] = df_nodes["root_id"].map(id2idx)

    print(f"Built node features: {len(df_nodes)} nodes")
    return df_nodes, tag_mlb, id2idx


def build_edge_features(data: Dict[str, Dict], id2idx: Dict[int, int]) -> pd.DataFrame:
    """Build edge feature matrix from connections."""
    df_connections = data["connections"]["df"].copy()
    df_connections["neuropil_no_side"] = df_connections["neuropil"].apply(
        lambda x: re.sub(r"_R|_L", "", x)
    )

    # Filter valid connections
    df_connections = df_connections[
        (df_connections["pre_root_id"].isin(id2idx)) &
        (df_connections["post_root_id"].isin(id2idx)) &
        (df_connections["neuropil_no_side"].notna()) &
        (df_connections["neuropil_no_side"] != "UNASGD")
    ].reset_index(drop=True)

    # Map to node indices
    df_connections["u"] = df_connections["pre_root_id"].map(id2idx)
    df_connections["v"] = df_connections["post_root_id"].map(id2idx)

    df_edges = df_connections[["u", "v", "neuropil", "nt_type", "syn_count"]].copy()
    df_edges = df_edges.rename(columns={"neuropil": "syn_neuropil", "nt_type": "syn_nt_type"})

    print(f"Built edge features: {len(df_edges)} edges")
    return df_edges


def encode_categories(df_nodes: pd.DataFrame, df_edges: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Dict, Dict]:
    """Encode categorical variables and create mappings."""
    category_mappings = {}
    reverse_mappings = {}

    # Node categories
    node_categories = ["nt_type", "primary_type", "super_class", "input_neuropil", "output_neuropil"]
    for cat in node_categories:
        df_nodes = df_nodes.dropna(subset=[cat])
        categories = sorted(df_nodes[cat].unique())
        df_nodes[cat] = df_nodes[cat].astype(pd.CategoricalDtype(categories, ordered=False))
        df_nodes[f"{cat}_code"] = df_nodes[cat].cat.codes

        category_mappings[cat] = dict(enumerate(categories))
        reverse_mappings[cat] = {v: k for k, v in category_mappings[cat].items()}

    # Edge categories
    edge_categories = ["syn_neuropil", "syn_nt_type"]
    for cat in edge_categories:
        df_edges = df_edges.dropna(subset=[cat])
        categories = sorted(df_edges[cat].unique())
        df_edges[cat] = df_edges[cat].astype(pd.CategoricalDtype(categories, ordered=False))
        df_edges[f"{cat}_code"] = df_edges[cat].cat.codes

        category_mappings[cat] = dict(enumerate(categories))
        reverse_mappings[cat] = {v: k for k, v in category_mappings[cat].items()}

    return df_nodes, df_edges, category_mappings, reverse_mappings


def prepare_features(df_nodes: pd.DataFrame) -> Tuple[pd.DataFrame, StandardScaler, StandardScaler]:
    """Scale features and prepare training data."""
    continuous_cols = ["centroid_x", "centroid_y", "centroid_z", "length_nm", "size_nm", "area_nm"]
    count_cols = ["input_synapses_count", "input_partners_count", "output_synapses_count", "output_partners_count"]

    # Split data
    indices = np.arange(len(df_nodes))
    train_idx, temp_idx = train_test_split(
        indices, test_size=0.3, stratify=df_nodes["super_class_code"], random_state=RANDOM_STATE
    )
    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=RANDOM_STATE)

    train_mask = np.zeros(len(df_nodes), dtype=bool)
    val_mask = np.zeros(len(df_nodes), dtype=bool)
    test_mask = np.zeros(len(df_nodes), dtype=bool)
    train_mask[train_idx] = True
    val_mask[val_idx] = True
    test_mask[test_idx] = True

    # Scale features
    scaler_cont = StandardScaler()
    scaler_counts = StandardScaler()

    scaler_cont.fit(df_nodes.loc[train_mask, continuous_cols])
    scaler_counts.fit(np.log1p(df_nodes.loc[train_mask, count_cols]))

    df_scaled = df_nodes.copy()
    df_scaled[continuous_cols] = scaler_cont.transform(df_nodes[continuous_cols])
    df_scaled[count_cols] = scaler_counts.transform(np.log1p(df_nodes[count_cols]))

    return df_scaled, scaler_cont, scaler_counts, train_mask, val_mask, test_mask


def create_pytorch_data(df_nodes: pd.DataFrame, df_edges: pd.DataFrame,
                       train_mask: np.ndarray, val_mask: np.ndarray, test_mask: np.ndarray) -> Data:
    """Create PyTorch Geometric Data object."""
    continuous_cols = ["centroid_x", "centroid_y", "centroid_z", "length_nm", "size_nm", "area_nm"]
    count_cols = ["input_synapses_count", "input_partners_count", "output_synapses_count", "output_partners_count"]
    onehot_cols = ['side_center', 'side_left', 'side_right', 'flow_afferent', 'flow_efferent', 'flow_intrinsic']
    tag_cols = [col for col in df_nodes.columns if col.startswith('tag_')]

    # Prepare features
    feat_cols = continuous_cols + count_cols + onehot_cols
    X_num = torch.tensor(df_nodes[feat_cols].values, dtype=torch.float32)
    input_np = torch.tensor(df_nodes["input_neuropil_code"].values, dtype=torch.long)
    output_np = torch.tensor(df_nodes["output_neuropil_code"].values, dtype=torch.long)

    # Prepare targets
    y_super = torch.tensor(df_nodes["super_class_code"].values, dtype=torch.long)
    y_nt = torch.tensor(df_nodes["nt_type_code"].values, dtype=torch.long)
    y_tags = torch.tensor(df_nodes[tag_cols].values, dtype=torch.float32)

    # Encode primary type
    primary_encoder = LabelEncoder()
    y_primary = torch.tensor(primary_encoder.fit_transform(df_nodes["primary_type"]), dtype=torch.long)

    # Prepare edges
    edge_index = torch.stack([
        torch.tensor(df_edges['u'].values, dtype=torch.long),
        torch.tensor(df_edges['v'].values, dtype=torch.long)
    ])
    edge_sc = torch.tensor(df_edges['syn_count'].values, dtype=torch.float32)
    edge_np = torch.tensor(df_edges['syn_neuropil_code'].values, dtype=torch.long)
    edge_nt = torch.tensor(df_edges['syn_nt_type_code'].values, dtype=torch.long)

    return Data(
        x=X_num, edge_index=edge_index, input_np=input_np, output_np=output_np,
        edge_sc=edge_sc, edge_np=edge_np, edge_nt=edge_nt,
        y_super=y_super, y_nt=y_nt, y_tags=y_tags, y_primary=y_primary,
        train_mask=torch.tensor(train_mask), val_mask=torch.tensor(val_mask), test_mask=torch.tensor(test_mask)
    )


class GNNModel(nn.Module):
    """Graph Neural Network for multi-task neuron classification."""

    def __init__(self, in_feats_dim: int, hidden_dim: int, embed_dim_node: int, embed_dim_edge: int,
                 num_in_np: int, num_out_np: int, num_edge_np: int, num_edge_nt: int,
                 num_super_classes: int, num_nt_classes: int, num_tag_classes: int,
                 num_primary_classes: int, dropout: float = 0.1):
        super().__init__()

        # Embeddings
        self.in_np_emb = nn.Embedding(num_in_np, embed_dim_node)
        self.out_np_emb = nn.Embedding(num_out_np, embed_dim_node)
        self.edge_np_emb = nn.Embedding(num_edge_np, embed_dim_edge)
        self.edge_nt_emb = nn.Embedding(num_edge_nt, embed_dim_edge)

        # Graph convolution
        edge_feat_dim = embed_dim_edge + embed_dim_edge + 1
        node_feat_dim = in_feats_dim + 2 * embed_dim_node
        self.edge_mlp = nn.Sequential(
            nn.Linear(edge_feat_dim, 64),
            nn.ReLU(),
            nn.Linear(64, node_feat_dim * hidden_dim)
        )
        self.conv = NNConv(node_feat_dim, hidden_dim, self.edge_mlp, aggr='mean')
        self.dropout = dropout

        # Output heads
        self.head_super = nn.Linear(hidden_dim, num_super_classes)
        self.head_nt = nn.Linear(hidden_dim, num_nt_classes)
        self.head_tags = nn.Linear(hidden_dim, num_tag_classes)
        self.head_primary = nn.Linear(hidden_dim, num_primary_classes)

    def forward(self, batch):
        # Node features
        x_num = batch.x
        x_in = self.in_np_emb(batch.input_np)
        x_out = self.out_np_emb(batch.output_np)
        x = torch.cat([x_num, x_in, x_out], dim=1)

        # Edge features
        e_nt = self.edge_nt_emb(batch.edge_nt)
        e_np = self.edge_np_emb(batch.edge_np)
        e_sc = batch.edge_sc.view(-1, 1)
        edge_attr = torch.cat([e_nt, e_np, e_sc], dim=1)

        # Graph convolution
        x = F.relu(self.conv(x, batch.edge_index, edge_attr))
        x = F.dropout(x, p=self.dropout, training=self.training)

        return (self.head_super(x), self.head_nt(x), self.head_tags(x), self.head_primary(x))


def train_model(model: GNNModel, data: Data, device: torch.device) -> GNNModel:
    """Train the GNN model."""
    model = model.to(device)
    data = data.to(device)

    # Setup training
    train_indices = data.train_mask.nonzero(as_tuple=False).view(-1)
    y_super_train = data.y_super[train_indices].cpu().numpy()
    y_nt_train = data.y_nt[train_indices].cpu().numpy()

    # Class weights
    super_weights = compute_class_weight('balanced', classes=np.unique(y_super_train), y=y_super_train)
    nt_weights = compute_class_weight('balanced', classes=np.unique(y_nt_train), y=y_nt_train)

    # Loss functions
    criterion_super = nn.CrossEntropyLoss(weight=torch.tensor(super_weights, dtype=torch.float32, device=device))
    criterion_nt = nn.CrossEntropyLoss(weight=torch.tensor(nt_weights, dtype=torch.float32, device=device))
    criterion_tags = nn.BCEWithLogitsLoss()
    criterion_primary = nn.CrossEntropyLoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Data loaders
    train_loader = NeighborLoader(data, input_nodes=data.train_mask, num_neighbors=[10, 5],
                                batch_size=BATCH_SIZE, shuffle=True)
    val_loader = NeighborLoader(data, input_nodes=data.val_mask, num_neighbors=[10, 5],
                              batch_size=BATCH_SIZE*2, shuffle=False)

    # Training loop
    print(f"Training for {EPOCHS} epochs...")
    for epoch in range(1, EPOCHS + 1):
        model.train()
        total_loss = 0

        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()

            out_super, out_nt, out_tags, out_primary = model(batch)
            n_center = batch.batch_size

            loss_super = criterion_super(out_super[:n_center], batch.y_super[:n_center])
            loss_nt = criterion_nt(out_nt[:n_center], batch.y_nt[:n_center])
            loss_tags = criterion_tags(out_tags[:n_center], batch.y_tags[:n_center])
            loss_primary = criterion_primary(out_primary[:n_center], batch.y_primary[:n_center])

            loss = loss_super + loss_nt + loss_tags + loss_primary
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        # Validation
        if epoch % 10 == 0:
            model.eval()
            val_metrics = evaluate_model(model, val_loader, device)
            print(f"Epoch {epoch} | Loss: {total_loss/len(train_loader):.4f} | "
                  f"Val Acc - Super: {val_metrics['super_acc']:.3f}, NT: {val_metrics['nt_acc']:.3f}, "
                  f"Tags: {val_metrics['tags_acc']:.3f}, Primary: {val_metrics['primary_acc']:.3f}")

    return model


def evaluate_model(model: GNNModel, data_loader: NeighborLoader, device: torch.device) -> Dict[str, float]:
    """Evaluate model performance."""
    model.eval()
    all_preds = {'super': [], 'nt': [], 'tags': [], 'primary': []}
    all_true = {'super': [], 'nt': [], 'tags': [], 'primary': []}

    with torch.no_grad():
        for batch in data_loader:
            batch = batch.to(device)
            out_super, out_nt, out_tags, out_primary = model(batch)
            n_center = batch.batch_size

            all_preds['super'].append(out_super[:n_center].argmax(dim=1).cpu().numpy())
            all_preds['nt'].append(out_nt[:n_center].argmax(dim=1).cpu().numpy())
            all_preds['tags'].append((torch.sigmoid(out_tags[:n_center]) > 0.5).cpu().numpy())
            all_preds['primary'].append(out_primary[:n_center].argmax(dim=1).cpu().numpy())

            all_true['super'].append(batch.y_super[:n_center].cpu().numpy())
            all_true['nt'].append(batch.y_nt[:n_center].cpu().numpy())
            all_true['tags'].append(batch.y_tags[:n_center].cpu().numpy().astype(int))
            all_true['primary'].append(batch.y_primary[:n_center].cpu().numpy())

    # Concatenate results
    for key in all_preds:
        all_preds[key] = np.concatenate(all_preds[key])
        all_true[key] = np.concatenate(all_true[key])

    return {
        'super_acc': accuracy_score(all_true['super'], all_preds['super']),
        'nt_acc': accuracy_score(all_true['nt'], all_preds['nt']),
        'tags_acc': accuracy_score(all_true['tags'], all_preds['tags']),
        'primary_acc': accuracy_score(all_true['primary'], all_preds['primary']),
        'super_f1': f1_score(all_true['super'], all_preds['super'], average='macro'),
        'nt_f1': f1_score(all_true['nt'], all_preds['nt'], average='macro'),
        'tags_f1': f1_score(all_true['tags'], all_preds['tags'], average='macro'),
        'primary_f1': f1_score(all_true['primary'], all_preds['primary'], average='macro', zero_division=0)
    }


class NeuronInferenceModel(torch.nn.Module):
    """Inference wrapper for the trained GNN model."""

    def __init__(self, model: GNNModel, scaler_cont: StandardScaler, scaler_counts: StandardScaler,
                 category_mappings: Dict, reverse_mappings: Dict, tag_mlb: MultiLabelBinarizer, device: str = 'cpu'):
        super().__init__()
        self.model = model.to(device)
        self.scaler_cont = scaler_cont
        self.scaler_counts = scaler_counts
        self.tag_mlb = tag_mlb
        self.device = device

        # Fix mappings
        self.category_mappings = {cat: {int(k): v for k, v in mapping.items()}
                                for cat, mapping in category_mappings.items()}
        self.reverse_mappings = {cat: {str(k): int(v) for k, v in mapping.items()}
                               for cat, mapping in reverse_mappings.items()}

        # Feature columns
        self.continuous_cols = ["centroid_x", "centroid_y", "centroid_z", "length_nm", "size_nm", "area_nm"]
        self.count_cols = ["input_synapses_count", "input_partners_count", "output_synapses_count", "output_partners_count"]
        self.onehot_cols = ['side_center', 'side_left', 'side_right', 'flow_afferent', 'flow_efferent', 'flow_intrinsic']
        self.cat_cols = ["nt_type", "super_class", "primary_type", "input_neuropil", "output_neuropil"]

    def predict(self, inputs):
        """Make predictions on input data."""
        if isinstance(inputs, dict):
            inputs = [inputs]
        df = pd.DataFrame(inputs).reset_index(drop=True)

        # Fill missing columns with defaults
        for col in self.continuous_cols:
            if col not in df:
                df[col] = float(self.scaler_cont.mean_[self.continuous_cols.index(col)])
        for col in self.count_cols + self.onehot_cols:
            if col not in df:
                df[col] = 0
        for col in self.cat_cols:
            if col not in df:
                df[col] = "Unknown"

        # Type conversions
        df[self.continuous_cols] = df[self.continuous_cols].astype(float)
        df[self.count_cols] = df[self.count_cols].astype(float)
        df[self.onehot_cols] = df[self.onehot_cols].astype(int)
        df[self.cat_cols] = df[self.cat_cols].astype(str)

        # Encode categories
        for cat in self.cat_cols:
            mapping = self.reverse_mappings.get(cat, {})
            df[f"{cat}_code"] = df[cat].map(lambda x: mapping.get(str(x), 0)).astype(int)

        # Scale features
        df[self.continuous_cols] = self.scaler_cont.transform(df[self.continuous_cols])
        df[self.count_cols] = self.scaler_counts.transform(np.log1p(df[self.count_cols]))

        # Create tensors
        feat_cols = self.continuous_cols + self.count_cols + self.onehot_cols
        X_num = torch.tensor(df[feat_cols].values, dtype=torch.float32, device=self.device)
        input_np = torch.tensor(df["input_neuropil_code"].values, dtype=torch.long, device=self.device)
        output_np = torch.tensor(df["output_neuropil_code"].values, dtype=torch.long, device=self.device)

        # Create dummy batch for isolated inference
        class Batch: pass
        batch = Batch()
        batch.x = X_num
        batch.input_np = input_np
        batch.output_np = output_np
        batch.edge_index = torch.zeros((2, 0), dtype=torch.long, device=self.device)
        batch.edge_sc = torch.zeros((0, 1), dtype=torch.float32, device=self.device)
        batch.edge_np = torch.zeros((0,), dtype=torch.long, device=self.device)
        batch.edge_nt = torch.zeros((0,), dtype=torch.long, device=self.device)

        # Make predictions
        self.model.eval()
        with torch.no_grad():
            out_super, out_nt, out_tags, out_primary = self.model(batch)

        # Convert predictions to labels
        pred_super = out_super.argmax(dim=1).cpu().numpy()
        pred_nt = out_nt.argmax(dim=1).cpu().numpy()
        pred_tags_binary = (torch.sigmoid(out_tags) > 0.5).cpu().numpy()
        pred_primary = out_primary.argmax(dim=1).cpu().numpy()

        # Format tag predictions
        tag_predictions = []
        for tag_vector in pred_tags_binary:
            active_tags = [self.tag_mlb.classes_[j] for j, is_active in enumerate(tag_vector) if is_active]
            tag_predictions.append(','.join(active_tags) if active_tags else 'no_tags')

        # Add predictions to dataframe
        df_out = df.copy()
        df_out["pred_super_class"] = [self.category_mappings["super_class"].get(int(c), f"Unknown_{c}") for c in pred_super]
        df_out["pred_nt_type"] = [self.category_mappings["nt_type"].get(int(c), f"Unknown_{c}") for c in pred_nt]
        df_out["pred_connectivity_tag"] = tag_predictions
        df_out["pred_primary_type"] = [self.category_mappings["primary_type"].get(int(c), f"Unknown_{c}") for c in pred_primary]

        return df_out

    def save(self, path: str):
        """Save the inference model."""
        torch.save(self, path)

    @staticmethod
    def load(path: str, device: str = 'cpu'):
        """Load a saved inference model."""
        model = torch.load(path, map_location=device)
        model.device = device
        model.model = model.model.to(device)
        return model


def save_artifacts(model: GNNModel, scaler_cont: StandardScaler, scaler_counts: StandardScaler,
                  category_mappings: Dict, reverse_mappings: Dict, tag_mlb: MultiLabelBinarizer,
                  df_nodes: pd.DataFrame, device: torch.device):
    """Save model and preprocessing artifacts."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # Create inference wrapper
    inference_model = NeuronInferenceModel(
        model, scaler_cont, scaler_counts, category_mappings, reverse_mappings, tag_mlb, device
    )

    # Save model
    model_path = os.path.join(OUTPUT_DIR, 'neuron_model_bundle.pt')
    inference_model.save(model_path)

    # Save sample data
    continuous_cols = ["centroid_x", "centroid_y", "centroid_z", "length_nm", "size_nm", "area_nm"]
    count_cols = ["input_synapses_count", "input_partners_count", "output_synapses_count", "output_partners_count"]
    onehot_cols = ['side_center', 'side_left', 'side_right', 'flow_afferent', 'flow_efferent', 'flow_intrinsic']

    sample_cols = continuous_cols + count_cols + onehot_cols + ["input_neuropil", "output_neuropil"]
    sample_data = df_nodes.sample(10, random_state=1)[sample_cols].reset_index(drop=True)
    sample_data['id'] = sample_data.index

    sample_path = os.path.join(OUTPUT_DIR, 'sample_data.json')
    sample_data.to_json(sample_path, orient='records', indent=4)

    print(f"Saved model to {model_path}")
    print(f"Saved sample data to {sample_path}")


def main():
    """Main execution pipeline."""
    print("=== Neuron Classification Pipeline ===")

    # Setup
    device = setup_environment()

    # Load and process data
    print("\n1. Loading datasets...")
    data = load_datasets()

    print("\n2. Building features...")
    df_nodes, tag_mlb, id2idx = build_node_features(data)
    df_edges = build_edge_features(data, id2idx)
    df_nodes, df_edges, category_mappings, reverse_mappings = encode_categories(df_nodes, df_edges)

    print("\n3. Preparing training data...")
    df_nodes_scaled, scaler_cont, scaler_counts, train_mask, val_mask, test_mask = prepare_features(df_nodes)
    pytorch_data = create_pytorch_data(df_nodes_scaled, df_edges, train_mask, val_mask, test_mask)

    # Create model
    print("\n4. Creating model...")
    model = GNNModel(
        in_feats_dim=pytorch_data.x.shape[1],
        hidden_dim=HIDDEN_DIM,
        embed_dim_node=EMBED_DIM_NODE,
        embed_dim_edge=EMBED_DIM_EDGE,
        num_in_np=int(df_nodes_scaled['input_neuropil_code'].max() + 1),
        num_out_np=int(df_nodes_scaled['output_neuropil_code'].max() + 1),
        num_edge_np=int(df_edges['syn_neuropil_code'].max() + 1),
        num_edge_nt=int(df_edges['syn_nt_type_code'].max() + 1),
        num_super_classes=int(df_nodes_scaled['super_class_code'].max() + 1),
        num_nt_classes=int(df_nodes_scaled['nt_type_code'].max() + 1),
        num_tag_classes=len([col for col in df_nodes.columns if col.startswith('tag_')]),
        num_primary_classes=len(df_nodes['primary_type'].unique()),
        dropout=DROPOUT
    )

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Train model
    print("\n5. Training model...")
    trained_model = train_model(model, pytorch_data, device)

    # Evaluate
    print("\n6. Final evaluation...")
    test_loader = NeighborLoader(pytorch_data, input_nodes=pytorch_data.test_mask,
                               num_neighbors=[10, 5], batch_size=BATCH_SIZE*2, shuffle=False)
    test_metrics = evaluate_model(trained_model, test_loader, device)

    print("Test Results:")
    print(f"Super Class - Acc: {test_metrics['super_acc']:.3f}, F1: {test_metrics['super_f1']:.3f}")
    print(f"Neurotransmitter - Acc: {test_metrics['nt_acc']:.3f}, F1: {test_metrics['nt_f1']:.3f}")
    print(f"Connectivity Tags - Acc: {test_metrics['tags_acc']:.3f}, F1: {test_metrics['tags_f1']:.3f}")
    print(f"Primary Type - Acc: {test_metrics['primary_acc']:.3f}, F1: {test_metrics['primary_f1']:.3f}")

    # Save artifacts
    print("\n7. Saving artifacts...")
    save_artifacts(trained_model, scaler_cont, scaler_counts, category_mappings,
                  reverse_mappings, tag_mlb, df_nodes, device)

    print("\n=== Pipeline Complete ===")


if __name__ == "__main__":
    main()

